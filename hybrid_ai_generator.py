#!/usr/bin/env python3
"""
G√©n√©rateur IA Hybride - MISTRAL (raisonnement) + CODELLAMA (code)
Architecture 2-passes:
1. Mistral analyse et d√©compose la requ√™te
2. CodeLlama g√©n√®re le code Three.js complexe
"""

import sys
import os
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import InferenceClient

ISOL_PATH = Path("/home/belikan/Isol")
sys.path.insert(0, str(ISOL_PATH / "kibali-IA"))

class HybridAIGenerator:
    """Syst√®me hybride: Mistral (pens√©e) + CodeLlama (ex√©cution)"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üß† Initialisation G√©n√©rateur Hybride IA...")
        print(f"   Device: {self.device}")
        
        # 1Ô∏è‚É£ MISTRAL - Raisonnement et analyse
        self.mistral_client = InferenceClient(token=os.getenv("HF_TOKEN"))
        self.mistral_model = "mistralai/Mistral-7B-Instruct-v0.2"
        print(f"‚úÖ Mistral charg√© (API HF) - Raisonnement")
        
        # 2Ô∏è‚É£ CODELLAMA - G√©n√©ration de code
        self.codellama = None
        self.codellama_tokenizer = None
        self._load_codellama()
    
    def _load_codellama(self):
        """Charge CodeLlama localement"""
        try:
            codellama_path = Path("/home/belikan/Isol/kibali-IA/kibali_data/models/huggingface_cache/models--codellama--CodeLlama-7b-hf/snapshots/6c284d1468fe6c413cf56183e69b194dcfa27fe6")
            
            if not codellama_path.exists():
                print("‚ö†Ô∏è  CodeLlama: fallback vers API HF")
                return
            
            print(f"üì¶ Chargement CodeLlama-7b...")
            
            self.codellama_tokenizer = AutoTokenizer.from_pretrained(
                str(codellama_path),
                trust_remote_code=True,
                local_files_only=True
            )
            
            self.codellama = AutoModelForCausalLM.from_pretrained(
                str(codellama_path),
                trust_remote_code=True,
                local_files_only=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                self.codellama = self.codellama.to(self.device)
            
            self.codellama.eval()
            print(f"‚úÖ CodeLlama charg√© sur {self.device} - G√©n√©ration code")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  CodeLlama fallback API: {e}")
            self.codellama = None
    
    def analyze_with_mistral(self, prompt, scene_context=None):
        """PHASE 1: Mistral analyse et d√©compose la requ√™te avec contexte sc√®ne"""
        
        # üî• NOUVEAU: Construit un prompt enrichi avec le contexte
        context_info = ""
        if scene_context and scene_context.get('total_objects', 0) > 0:
            context_info = f"""

CONTEXTE DE LA SC√àNE ACTUELLE:
- Nombre d'objets existants: {scene_context['total_objects']}
- Objets pr√©sents: {', '.join([obj['name'] for obj in scene_context.get('objects', [])[:5]])}
- Personnage pr√©sent: {'OUI' if scene_context.get('has_character') else 'NON'}
- V√©hicule pr√©sent: {'OUI' if scene_context.get('has_vehicle') else 'NON'}
- Eau pr√©sente: {'OUI' if scene_context.get('has_water') else 'NON'}
- Environnement pr√©sent: {'OUI' if scene_context.get('has_environment') else 'NON'}

IMPORTANT: Tiens compte du contexte pour adapter ta g√©n√©ration!
- Si un bateau existe et l'utilisateur demande "ajoute de l'eau", positionne l'eau SOUS le bateau
- Si un personnage existe et l'utilisateur demande "ajoute un sol", cr√©e le sol SOUS le personnage
- Adapte la position, l'√©chelle et l'orientation selon les objets existants
- Si demande de "mettre X sur Y", utilise les positions des objets existants pour calculer la nouvelle position"""
        
        analysis_prompt = f"""Analyse cette requ√™te 3D et fournis une analyse technique d√©taill√©e en JSON:{context_info}

REQU√äTE: "{prompt}"

Analyse technique professionnelle:
- object_type: character/vehicle/building/furniture/animal/plant/environment/props/mechanical/water/terrain
- style: realistic/stylized/cartoon/anime/cyberpunk/fantasy/medieval/modern/abstract/minimalist
- complexity: simple/medium/complex/very_complex (bas√© sur nombre de parties et d√©tails)
- key_features: liste d√©taill√©e des caract√©ristiques techniques (dimensions, mat√©riaux, fonctionnalit√©s)
- geometry_hints: g√©om√©tries Three.js optimales (BoxGeometry, CylinderGeometry, SphereGeometry, ConeGeometry, TorusGeometry, PlaneGeometry, etc.)
- color_palette: couleurs hexad√©cimales r√©alistes pour mat√©riaux PBR
- material_properties: {{"metalness": float, "roughness": float, "transmission": float, "transparent": bool}} par partie
- scale_reference: √©chelle r√©aliste en m√®tres (ex: character=1.8, vehicle=4.5, water_plane=100)
- position_hint: position sugg√©r√©e bas√©e sur le contexte (ex: {{"y": -0.5}} pour eau sous bateau)
- animation_potential: parties animables (joints, portes, roues, vagues, etc.)
- lighting_requirements: besoins en √©clairage sp√©cifiques
- contextual_adaptation: comment s'int√©grer dans la sc√®ne existante

R√©ponds UNIQUEMENT en JSON valide et d√©taill√©."""

        try:
            # Utilise chat_completion au lieu de text_generation
            messages = [
                {
                    "role": "system",
                    "content": "Tu es un expert en analyse de prompts 3D. R√©ponds uniquement en JSON valide."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=256,
                temperature=0.3
            )
            
            # Extrait le contenu de la r√©ponse
            response_text = response.choices[0].message.content.strip()
            
            # Parse la r√©ponse JSON
            import json
            # Extrait le JSON de la r√©ponse
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                analysis = json.loads(response_text[json_start:json_end])
                return analysis
            else:
                # Fallback si pas de JSON
                return {
                    'object_type': 'object',
                    'style': 'realistic',
                    'complexity': 'medium',
                    'key_features': [prompt],
                    'geometry_hints': ['BoxGeometry', 'CylinderGeometry', 'SphereGeometry'],
                    'color_palette': ['0x888888', '0x444444', '0xcccccc'],
                    'material_properties': {'metalness': 0.3, 'roughness': 0.7, 'transmission': 0.0},
                    'scale_reference': 1.0,
                    'animation_potential': [],
                    'lighting_requirements': 'standard'
                }
        except Exception as e:
            print(f"‚ö†Ô∏è  Mistral analysis error: {e}")
            return {
                'object_type': 'object',
                'style': 'realistic',
                'complexity': 'medium',
                'key_features': [prompt],
                'geometry_hints': ['BoxGeometry', 'CylinderGeometry'],
                'color_palette': ['0x888888', '0x666666'],
                'material_properties': {'metalness': 0.2, 'roughness': 0.8, 'transmission': 0.0},
                'scale_reference': 1.0,
                'animation_potential': [],
                'lighting_requirements': 'standard'
            }
    
    def _get_example_for_type(self, object_type, scene_context=None):
        """Retourne un exemple de code selon le type d'objet"""
        
        # Position adaptative selon contexte
        position_y = 0
        if scene_context and scene_context.get('has_vehicle'):
            position_y = -1  # Sous le v√©hicule pour eau/sol
        
        examples = {
            'water': f"""
const waterGroup = new THREE.Group();
const waterGeo = new THREE.PlaneGeometry(50, 50, 32, 32);
const waterMat = new THREE.MeshStandardMaterial({{
    color: 0x1e90ff,
    metalness: 0.8,
    roughness: 0.2,
    transparent: true,
    opacity: 0.7
}});
const waterMesh = new THREE.Mesh(waterGeo, waterMat);
waterMesh.rotation.x = -Math.PI / 2;
waterMesh.position.y = {position_y};
waterGroup.add(waterMesh);
studio.scene.add(waterGroup);
""",
            'vehicle': """
const boatGroup = new THREE.Group();
const hullGeo = new THREE.BoxGeometry(4, 1, 2);
const hullMat = new THREE.MeshStandardMaterial({{ color: 0x8b4513, roughness: 0.7 }});
const hull = new THREE.Mesh(hullGeo, hullMat);
boatGroup.add(hull);
const deckGeo = new THREE.BoxGeometry(3.5, 0.2, 1.8);
const deck = new THREE.Mesh(deckGeo, hullMat);
deck.position.y = 0.6;
boatGroup.add(deck);
studio.scene.add(boatGroup);
""",
            'character': """
const characterGroup = new THREE.Group();
const bodyGeo = new THREE.CylinderGeometry(0.3, 0.3, 1.2, 16);
const bodyMat = new THREE.MeshStandardMaterial({{ color: 0xffdbac }});
const body = new THREE.Mesh(bodyGeo, bodyMat);
characterGroup.add(body);
const headGeo = new THREE.SphereGeometry(0.25, 16, 16);
const head = new THREE.Mesh(headGeo, bodyMat);
head.position.y = 0.85;
characterGroup.add(head);
studio.scene.add(characterGroup);
""",
            'environment': f"""
const groundGroup = new THREE.Group();
const groundGeo = new THREE.PlaneGeometry(100, 100);
const groundMat = new THREE.MeshStandardMaterial({{ color: 0x228b22, roughness: 0.9 }});
const ground = new THREE.Mesh(groundGeo, groundMat);
ground.rotation.x = -Math.PI / 2;
ground.position.y = {position_y};
groundGroup.add(ground);
studio.scene.add(groundGroup);
"""
        }
        
        return examples.get(object_type, examples['vehicle'])
    
    def generate_code_with_codellama(self, prompt, analysis, scene_context=None):
        """PHASE 2: G√©n√®re le code Three.js avec Mistral API ou CodeLlama local + contexte sc√®ne"""
        
        # üî• NOUVEAU: Construit les instructions contextuelles
        context_instructions = ""
        if scene_context and scene_context.get('total_objects', 0) > 0:
            existing_objects = ', '.join([f"{obj['name']} (type: {obj['type']}, pos: {obj['position']})" 
                                         for obj in scene_context.get('objects', [])[:3]])
            
            context_instructions = f"""

CONTEXTE DE LA SC√àNE EXISTANTE - TR√àS IMPORTANT:
- {scene_context['total_objects']} objet(s) d√©j√† dans la sc√®ne
- Objets existants: {existing_objects}
- Bounds sc√®ne: min({scene_context['bounds']['min']['x']:.1f}, {scene_context['bounds']['min']['y']:.1f}, {scene_context['bounds']['min']['z']:.1f}), max({scene_context['bounds']['max']['x']:.1f}, {scene_context['bounds']['max']['y']:.1f}, {scene_context['bounds']['max']['z']:.1f})

R√àGLES DE POSITIONNEMENT CONTEXTUEL:
1. Si demande "ajouter eau" et v√©hicule existe ‚Üí positionne eau SOUS le v√©hicule (y n√©gatif)
2. Si demande "ajouter sol/terrain" ‚Üí positionne sous tous les objets existants
3. Si demande "mettre X sur Y" ‚Üí calcule position relative bas√©e sur les objets existants
4. Utilise les bounds pour ne pas cr√©er d'objets en collision
5. Adapte l'√©chelle en fonction des objets existants

Position hint: {analysis.get('position_hint', 'auto')}
Adaptation contextuelle: {analysis.get('contextual_adaptation', 'int√©gration intelligente')}"""
        
        # PRIORIT√â: Utilise Mistral API pour g√©n√©rer du vrai code cr√©atif
        print(f"   üíª G√©n√©ration code contextuel avec Mistral API...")
        
        # üî• EXEMPLE CONCRET selon le type d'objet
        example_code = self._get_example_for_type(analysis.get('object_type', 'object'), scene_context)
        
        # Position adapt√©e au contexte
        position_hint = "0"
        if scene_context and scene_context.get('total_objects', 0) > 0:
            if 'water' in prompt.lower() or 'sea' in prompt.lower() or 'mer' in prompt.lower():
                position_hint = "-1"  # Sous les objets existants
            elif 'ground' in prompt.lower() or 'sol' in prompt.lower() or 'floor' in prompt.lower():
                position_hint = "-0.5"  # Sous les objets
        
        code_prompt = f"""Create Three.js code for: {prompt}

Type: {analysis.get('object_type')}
Colors: {', '.join(analysis.get('color_palette', ['0x888888'])[:2])}
Scale: {analysis.get('scale_reference', 1.0)}m
Position Y: {position_hint}

RULES:
- NO import/require/export
- Use THREE (global)
- Use studio.scene.add()
- ASCII characters only

EXAMPLE CODE:
{example_code}

NOW CREATE CODE FOR: {prompt}

CODE:"""

        try:
            # Utilise chat_completion au lieu de text_generation pour Mistral
            from huggingface_hub import InferenceClient
            
            messages = [
                {
                    "role": "user",
                    "content": code_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=2000,  # Augment√© pour code plus long
                temperature=0.9,  # Plus cr√©atif
                top_p=0.95
            )
            
            # Extrait le contenu de la r√©ponse
            code = response.choices[0].message.content.strip()
            
            # Nettoyage agressif du code g√©n√©r√©
            # 1. Enl√®ve les balises markdown
            if '```' in code:
                parts = code.split('```')
                for i, part in enumerate(parts):
                    if i % 2 == 1:  # Code entre ```
                        if part.startswith('javascript\n') or part.startswith('js\n'):
                            code = part.split('\n', 1)[1]
                        else:
                            code = part
                        break
            
            # 2. üî• Nettoie caract√®res non-ASCII
            cleaned_chars = []
            for char in code:
                if ord(char) < 128 or char in ['\n', '\r', '\t']:  # ASCII seulement
                    cleaned_chars.append(char)
                else:
                    print(f"   üö´ Caract√®re non-ASCII supprim√©: {repr(char)}")
            code = ''.join(cleaned_chars)
            
            # 3. üî• Retire TOUTES les lignes avec import/require/export
            import re
            lines = []
            for line in code.split('\n'):
                stripped = line.strip()
                # √âlimine les imports/requires/exports
                if any(keyword in stripped for keyword in ['import ', 'require(', 'export ', 'module.exports']):
                    print(f"   üö´ Ligne supprim√©e: {stripped[:50]}")
                    continue
                lines.append(line)
            code = '\n'.join(lines)
            
            # 4. Retire les commentaires multi-lignes qui peuvent casser
            code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
            
            # 5. Retire les lignes de commentaires simples (sauf URLs)
            lines = []
            for line in code.split('\n'):
                stripped = line.strip()
                # Garde seulement si pas un commentaire pur (sauf si contient http)
                if not stripped.startswith('//') or 'http' in stripped:
                    lines.append(line)
            code = '\n'.join(lines)
            
            # 6. Fixe les parenth√®ses/accolades mal ferm√©es (basique)
            open_parens = code.count('(')
            close_parens = code.count(')')
            if open_parens > close_parens:
                code += ')' * (open_parens - close_parens)
            
            # 6. Ajoute addLog si manquant
            if 'addLog' not in code:
                code += f"\naddLog('‚úÖ {prompt} cr√©√©');"
            
            # V√©rifie que le code contient du Three.js
            if 'THREE.' in code or 'Group' in code:
                return code
            else:
                print(f"‚ö†Ô∏è  Code g√©n√©r√© invalide, fallback")
                return self._generate_fallback_code(prompt, analysis)
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Mistral API error: {e}")
            # Essaie CodeLlama local si disponible
            if self.codellama is not None:
                return self._generate_with_local_codellama(prompt, analysis)
            return self._generate_fallback_code(prompt, analysis)
    
    def _generate_with_local_codellama(self, prompt, analysis):
        """Utilise CodeLlama local"""
        
        # Construit un prompt enrichi avec l'analyse
        code_prompt = f"""// Three.js Expert Code Generator
// Task: Create {analysis.get('object_type', 'object')}
// Style: {analysis.get('style', 'realistic')}
// Complexity: {analysis.get('complexity', 'medium')}
// Features: {', '.join(analysis.get('key_features', []))}

// Generate THREE.js code for: {prompt}
// Use geometries: {', '.join(analysis.get('geometry_hints', []))}
// Color palette: {', '.join(str(c) for c in analysis.get('color_palette', []))}

const createModel = () => {{
    const group = new THREE.Group();
    
    // Materials
    const materials = {{
        main: new THREE.MeshStandardMaterial({{
            color: {analysis.get('color_palette', ['0x888888'])[0]},
            roughness: 0.7,
            metalness: 0.2
        }})
    }};
    
    // Geometry
"""
        
        if self.codellama is not None:
            # Utilise CodeLlama local
            try:
                inputs = self.codellama_tokenizer(code_prompt, return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    outputs = self.codellama.generate(
                        **inputs,
                        max_new_tokens=1024,  # Code long et complexe
                        temperature=0.4,
                        do_sample=True,
                        top_p=0.95,
                        repetition_penalty=1.1
                    )
                
                generated = self.codellama_tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extrait seulement le code g√©n√©r√© (apr√®s le prompt)
                code = generated[len(code_prompt):].strip()
                
                # Compl√®te le code s'il manque la fin
                if not code.endswith('};'):
                    code += "\n    return group;\n};\n\nconst model = createModel();\nstudio.scene.add(model);"
                
                return code
                
            except Exception as e:
                print(f"‚ö†Ô∏è  CodeLlama generation error: {e}")
        
        # Fallback: g√©n√©ration simple
        return self._generate_fallback_code(prompt, analysis)
    
    def _generate_fallback_code(self, prompt, analysis):
        """Code simple si CodeLlama √©choue"""
        obj_type = analysis.get('object_type', 'object')
        color = analysis.get('color_palette', ['0x888888'])[0]
        
        return f"""
const group = new THREE.Group();

// Create {obj_type}
const geometry = new THREE.BoxGeometry(1, 1, 1);
const material = new THREE.MeshStandardMaterial({{
    color: {color},
    roughness: 0.7,
    metalness: 0.2
}});
const mesh = new THREE.Mesh(geometry, material);
mesh.castShadow = true;
mesh.receiveShadow = true;

group.add(mesh);
studio.scene.add(group);

// Log
addLog('‚úÖ {prompt} cr√©√©');
"""
    
    def generate(self, prompt, object_type='object', scene_context=None):
        """Pipeline complet: Mistral analyse ‚Üí CodeLlama g√©n√®re avec contexte"""
        print(f"üß† [Mistral] Analyse de: {prompt}")
        
        # üî• NOUVEAU: Enrichit l'analyse avec le contexte de la sc√®ne
        if scene_context and scene_context.get('total_objects', 0) > 0:
            print(f"üìä Contexte sc√®ne: {scene_context['total_objects']} objet(s)")
            print(f"   ‚Ä¢ Personnage: {scene_context.get('has_character', False)}")
            print(f"   ‚Ä¢ V√©hicule: {scene_context.get('has_vehicle', False)}")
            print(f"   ‚Ä¢ Eau: {scene_context.get('has_water', False)}")
            print(f"   ‚Ä¢ Environnement: {scene_context.get('has_environment', False)}")
        
        analysis = self.analyze_with_mistral(prompt, scene_context)
        print(f"   ‚Üí Type: {analysis.get('object_type')}, Style: {analysis.get('style')}")
        
        print(f"üíª [CodeLlama] G√©n√©ration du code contextuel...")
        code = self.generate_code_with_codellama(prompt, analysis, scene_context)
        print(f"   ‚Üí {len(code)} caract√®res g√©n√©r√©s")
        
        return {
            'success': True,
            'code': code,
            'type': 'javascript',
            'analysis': analysis
        }
    
    def fix_code_with_mistral(self, broken_code, error_message, original_prompt):
        """AUTO-CORRECTION: Mistral corrige le code cass√©"""
        print(f"üîß [Mistral] Auto-correction du code...")
        print(f"   Erreur: {error_message}")
        
        # üî• NOUVEAU: D√©tection d'erreur sp√©cifique import/require
        if 'import' in error_message.lower() or 'require' in error_message.lower() or 'module' in error_message.lower():
            print("   üö´ D√©tection erreur import/require - Nettoyage automatique")
            # Nettoie directement sans appeler Mistral
            import re
            lines = []
            for line in broken_code.split('\n'):
                stripped = line.strip()
                if not any(keyword in stripped for keyword in ['import ', 'require(', 'export ', 'module.exports']):
                    lines.append(line)
            cleaned_code = '\n'.join(lines)
            
            # V√©rifie que le code restant est valide
            if 'THREE.' in cleaned_code and len(cleaned_code) > 50:
                print("   ‚úÖ Code nettoy√© automatiquement")
                return cleaned_code
        
        fix_prompt = f"""Tu es un expert JavaScript/Three.js. Corrige ce code qui g√©n√®re une erreur.

‚ö†Ô∏è R√àGLES CRITIQUES:
- ‚ùå INTERDICTION: import, require, export, module.exports
- ‚úÖ CODE NAVIGATEUR UNIQUEMENT (THREE d√©j√† disponible)
- ‚úÖ Utilise studio.scene pour ajouter les objets

ERREUR JAVASCRIPT:
{error_message}

CODE PROBL√âMATIQUE:
{broken_code[:500]}...

CONTEXTE: Le code devait cr√©er "{original_prompt}" en Three.js

INSTRUCTIONS:
1. Retire TOUS les import/require/export si pr√©sents
2. Utilise uniquement THREE.* (d√©j√† disponible globalement)
3. Corrige les erreurs de syntaxe (parenth√®ses, virgules, etc.)
4. Retourne UNIQUEMENT le code corrig√© complet et fonctionnel
5. Le code doit utiliser THREE.js et studio.scene
6. PAS de markdown, PAS de commentaires explicatifs

CODE CORRIG√â:"""

        try:
            messages = [
                {
                    "role": "system",
                    "content": "Tu es un expert en d√©bogage JavaScript. R√©ponds uniquement avec du code corrig√©."
                },
                {
                    "role": "user",
                    "content": fix_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=2000,
                temperature=0.3  # Moins cr√©atif, plus pr√©cis
            )
            
            fixed_code = response.choices[0].message.content.strip()
            
            # üî• Nettoyage renforc√© du code corrig√©
            import re
            # Retire markdown
            fixed_code = re.sub(r'```[a-z]*\n?', '', fixed_code)
            fixed_code = re.sub(r'/\*.*?\*/', '', fixed_code, flags=re.DOTALL)
            
            # Retire TOUS les imports/requires/exports
            lines = []
            for line in fixed_code.split('\n'):
                stripped = line.strip()
                if not any(keyword in stripped for keyword in ['import ', 'require(', 'export ', 'module.exports']):
                    lines.append(line)
                else:
                    print(f"   üö´ Ligne supprim√©e (correction): {stripped[:50]}")
            fixed_code = '\n'.join(lines)
            
            print(f"   ‚úÖ Code corrig√©: {len(fixed_code)} caract√®res")
            return fixed_code
            
        except Exception as e:
            print(f"   ‚ùå Correction √©chou√©e: {e}")
            return None


# Instance globale
_generator = None

def init_hybrid_generator():
    """Initialise le g√©n√©rateur hybride"""
    global _generator
    if _generator is None:
        _generator = HybridAIGenerator()
    return _generator

def generate_hybrid_3d(prompt, object_type='object'):
    """Point d'entr√©e principal"""
    generator = init_hybrid_generator()
    return generator.generate(prompt, object_type)

def fix_broken_code(code, error, prompt):
    """Point d'entr√©e pour auto-correction"""
    generator = init_hybrid_generator()
    fixed = generator.fix_code_with_mistral(code, error, prompt)
    if fixed:
        return {'success': True, 'fixed_code': fixed}
    else:
        return {'success': False, 'error': 'Auto-correction impossible'}

