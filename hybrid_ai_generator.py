#!/usr/bin/env python3
"""
G√©n√©rateur IA Hybride - MISTRAL (raisonnement) + CODELLAMA (code)
Architecture 2-passes:
1. Mistral analyse et d√©compose la requ√™te
2. CodeLlama g√©n√®re le code Three.js complexe
"""

import sys
import os
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import InferenceClient

ISOL_PATH = Path("/home/belikan/Isol")
sys.path.insert(0, str(ISOL_PATH / "kibali-IA"))

class HybridAIGenerator:
    """Syst√®me hybride: Mistral (pens√©e) + CodeLlama (ex√©cution)"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üß† Initialisation G√©n√©rateur Hybride IA...")
        print(f"   Device: {self.device}")
        
        # 1Ô∏è‚É£ MISTRAL - Raisonnement et analyse
        self.mistral_client = InferenceClient(token=os.getenv("HF_TOKEN"))
        self.mistral_model = "mistralai/Mistral-7B-Instruct-v0.2"
        print(f"‚úÖ Mistral charg√© (API HF) - Raisonnement")
        
        # 2Ô∏è‚É£ CODELLAMA - G√©n√©ration de code
        self.codellama = None
        self.codellama_tokenizer = None
        self._load_codellama()
    
    def _load_codellama(self):
        """Charge CodeLlama localement"""
        try:
            codellama_path = Path("/home/belikan/Isol/kibali-IA/kibali_data/models/huggingface_cache/models--codellama--CodeLlama-7b-hf/snapshots/6c284d1468fe6c413cf56183e69b194dcfa27fe6")
            
            if not codellama_path.exists():
                print("‚ö†Ô∏è  CodeLlama: fallback vers API HF")
                return
            
            print(f"üì¶ Chargement CodeLlama-7b...")
            
            self.codellama_tokenizer = AutoTokenizer.from_pretrained(
                str(codellama_path),
                trust_remote_code=True,
                local_files_only=True
            )
            
            self.codellama = AutoModelForCausalLM.from_pretrained(
                str(codellama_path),
                trust_remote_code=True,
                local_files_only=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                self.codellama = self.codellama.to(self.device)
            
            self.codellama.eval()
            print(f"‚úÖ CodeLlama charg√© sur {self.device} - G√©n√©ration code")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  CodeLlama fallback API: {e}")
            self.codellama = None
    
    def analyze_with_mistral(self, prompt):
        """PHASE 1: Mistral analyse et d√©compose la requ√™te"""
        analysis_prompt = f"""Analyse cette requ√™te 3D et fournis une structure JSON:

REQU√äTE: "{prompt}"

D√©termine:
- object_type: character/environment/object/effect
- style: realistic/cartoon/low-poly/cyberpunk/fantasy
- complexity: simple/medium/complex
- key_features: liste des caract√©ristiques principales
- geometry_hints: formes Three.js √† utiliser (BoxGeometry, SphereGeometry, etc.)
- color_palette: couleurs hexad√©cimales sugg√©r√©es

R√©ponds UNIQUEMENT en JSON valide."""

        try:
            # Utilise chat_completion au lieu de text_generation
            messages = [
                {
                    "role": "system",
                    "content": "Tu es un expert en analyse de prompts 3D. R√©ponds uniquement en JSON valide."
                },
                {
                    "role": "user",
                    "content": analysis_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=256,
                temperature=0.3
            )
            
            # Extrait le contenu de la r√©ponse
            response_text = response.choices[0].message.content.strip()
            
            # Parse la r√©ponse JSON
            import json
            # Extrait le JSON de la r√©ponse
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                analysis = json.loads(response_text[json_start:json_end])
                return analysis
            else:
                # Fallback si pas de JSON
                return {
                    'object_type': 'object',
                    'style': 'realistic',
                    'complexity': 'medium',
                    'key_features': [prompt],
                    'geometry_hints': ['BoxGeometry', 'SphereGeometry'],
                    'color_palette': ['0x888888']
                }
        except Exception as e:
            print(f"‚ö†Ô∏è  Mistral analysis error: {e}")
            return {
                'object_type': 'object',
                'style': 'realistic',
                'complexity': 'medium',
                'key_features': [prompt],
                'geometry_hints': ['BoxGeometry'],
                'color_palette': ['0x888888']
            }
    
    def generate_code_with_codellama(self, prompt, analysis):
        """PHASE 2: G√©n√®re le code Three.js avec Mistral API ou CodeLlama local"""
        
        # PRIORIT√â: Utilise Mistral API pour g√©n√©rer du vrai code cr√©atif
        print(f"   üíª G√©n√©ration code avec Mistral API...")
        
        code_prompt = f"""Tu es un expert Three.js. G√©n√®re du code JavaScript cr√©atif et d√©taill√© pour: "{prompt}"

ANALYSE:
- Type: {analysis.get('object_type', 'object')}
- Style: {analysis.get('style', 'realistic')}
- Complexit√©: {analysis.get('complexity', 'medium')}
- Features: {', '.join(analysis.get('key_features', []))}
- Couleurs sugg√©r√©es: {', '.join(analysis.get('color_palette', ['0x888888']))}

EXIGENCES CR√âATIVES:
1. Cr√©e un groupe principal: const group = new THREE.Group();
2. Pour un personnage: ajoute torse, t√™te, bras, jambes avec formes vari√©es (CylinderGeometry, SphereGeometry, BoxGeometry)
3. Pour environnement: multiple objets (arbres, rochers, sol) avec positions espac√©es
4. Pour objet: d√©tails complexes, plusieurs parties assembl√©es
5. Utilise MeshStandardMaterial avec couleurs contextuelles (metalness: 0.3, roughness: 0.7)
6. Positionne chaque √©l√©ment avec .position.set(x, y, z) et rotation si n√©cessaire
7. Ajoute le groupe: studio.scene.add(group);
8. Log final: addLog('‚úÖ {prompt} cr√©√© avec [nombre] √©l√©ments');

G√âN√àRE 30-50 LIGNES DE CODE MINIMUM, CR√âATIF ET D√âTAILL√â. CODE UNIQUEMENT, PAS DE MARKDOWN NI ```javascript:"""

        try:
            # Utilise chat_completion au lieu de text_generation pour Mistral
            from huggingface_hub import InferenceClient
            
            messages = [
                {
                    "role": "user",
                    "content": code_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=2000,  # Augment√© pour code plus long
                temperature=0.9,  # Plus cr√©atif
                top_p=0.95
            )
            
            # Extrait le contenu de la r√©ponse
            code = response.choices[0].message.content.strip()
            
            # Nettoyage agressif du code g√©n√©r√©
            # 1. Enl√®ve les balises markdown
            if '```' in code:
                parts = code.split('```')
                for i, part in enumerate(parts):
                    if i % 2 == 1:  # Code entre ```
                        if part.startswith('javascript\n') or part.startswith('js\n'):
                            code = part.split('\n', 1)[1]
                        else:
                            code = part
                        break
            
            # 2. Retire les commentaires multi-lignes qui peuvent casser
            import re
            code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
            
            # 3. Retire les lignes de commentaires simples
            lines = []
            for line in code.split('\n'):
                stripped = line.strip()
                # Garde seulement si pas un commentaire pur
                if not stripped.startswith('//') or 'http' in stripped:
                    lines.append(line)
            code = '\n'.join(lines)
            
            # 4. Fixe les parenth√®ses/accolades mal ferm√©es (basique)
            open_parens = code.count('(')
            close_parens = code.count(')')
            if open_parens > close_parens:
                code += ')' * (open_parens - close_parens)
            
            # 5. Ajoute addLog si manquant
            if 'addLog' not in code:
                code += f"\naddLog('‚úÖ {prompt} cr√©√©');"
            
            # V√©rifie que le code contient du Three.js
            if 'THREE.' in code or 'Group' in code:
                return code
            else:
                print(f"‚ö†Ô∏è  Code g√©n√©r√© invalide, fallback")
                return self._generate_fallback_code(prompt, analysis)
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Mistral API error: {e}")
            # Essaie CodeLlama local si disponible
            if self.codellama is not None:
                return self._generate_with_local_codellama(prompt, analysis)
            return self._generate_fallback_code(prompt, analysis)
    
    def _generate_with_local_codellama(self, prompt, analysis):
        """Utilise CodeLlama local"""
        
        # Construit un prompt enrichi avec l'analyse
        code_prompt = f"""// Three.js Expert Code Generator
// Task: Create {analysis.get('object_type', 'object')}
// Style: {analysis.get('style', 'realistic')}
// Complexity: {analysis.get('complexity', 'medium')}
// Features: {', '.join(analysis.get('key_features', []))}

// Generate THREE.js code for: {prompt}
// Use geometries: {', '.join(analysis.get('geometry_hints', []))}
// Color palette: {', '.join(str(c) for c in analysis.get('color_palette', []))}

const createModel = () => {{
    const group = new THREE.Group();
    
    // Materials
    const materials = {{
        main: new THREE.MeshStandardMaterial({{
            color: {analysis.get('color_palette', ['0x888888'])[0]},
            roughness: 0.7,
            metalness: 0.2
        }})
    }};
    
    // Geometry
"""
        
        if self.codellama is not None:
            # Utilise CodeLlama local
            try:
                inputs = self.codellama_tokenizer(code_prompt, return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    outputs = self.codellama.generate(
                        **inputs,
                        max_new_tokens=1024,  # Code long et complexe
                        temperature=0.4,
                        do_sample=True,
                        top_p=0.95,
                        repetition_penalty=1.1
                    )
                
                generated = self.codellama_tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extrait seulement le code g√©n√©r√© (apr√®s le prompt)
                code = generated[len(code_prompt):].strip()
                
                # Compl√®te le code s'il manque la fin
                if not code.endswith('};'):
                    code += "\n    return group;\n};\n\nconst model = createModel();\nstudio.scene.add(model);"
                
                return code
                
            except Exception as e:
                print(f"‚ö†Ô∏è  CodeLlama generation error: {e}")
        
        # Fallback: g√©n√©ration simple
        return self._generate_fallback_code(prompt, analysis)
    
    def _generate_fallback_code(self, prompt, analysis):
        """Code simple si CodeLlama √©choue"""
        obj_type = analysis.get('object_type', 'object')
        color = analysis.get('color_palette', ['0x888888'])[0]
        
        return f"""
const group = new THREE.Group();

// Create {obj_type}
const geometry = new THREE.BoxGeometry(1, 1, 1);
const material = new THREE.MeshStandardMaterial({{
    color: {color},
    roughness: 0.7,
    metalness: 0.2
}});
const mesh = new THREE.Mesh(geometry, material);
mesh.castShadow = true;
mesh.receiveShadow = true;

group.add(mesh);
studio.scene.add(group);

// Log
addLog('‚úÖ {prompt} cr√©√©');
"""
    
    def generate(self, prompt, object_type='object'):
        """Pipeline complet: Mistral analyse ‚Üí CodeLlama g√©n√®re"""
        print(f"üß† [Mistral] Analyse de: {prompt}")
        analysis = self.analyze_with_mistral(prompt)
        print(f"   ‚Üí Type: {analysis.get('object_type')}, Style: {analysis.get('style')}")
        
        print(f"üíª [CodeLlama] G√©n√©ration du code...")
        code = self.generate_code_with_codellama(prompt, analysis)
        print(f"   ‚Üí {len(code)} caract√®res g√©n√©r√©s")
        
        return {
            'success': True,
            'code': code,
            'type': 'javascript',
            'analysis': analysis
        }
    
    def fix_code_with_mistral(self, broken_code, error_message, original_prompt):
        """AUTO-CORRECTION: Mistral corrige le code cass√©"""
        print(f"üîß [Mistral] Auto-correction du code...")
        print(f"   Erreur: {error_message}")
        
        fix_prompt = f"""Tu es un expert JavaScript/Three.js. Corrige ce code qui g√©n√®re une erreur.

ERREUR JAVASCRIPT:
{error_message}

CODE PROBL√âMATIQUE:
{broken_code}

CONTEXTE: Le code devait cr√©er "{original_prompt}" en Three.js

INSTRUCTIONS:
1. Identifie l'erreur (syntaxe, parenth√®ses, virgules, etc.)
2. Corrige le code COMPL√àTEMENT
3. Retourne UNIQUEMENT le code corrig√©, sans explications
4. Le code doit utiliser THREE.js et studio.scene
5. PAS de markdown, PAS de commentaires explicatifs

CODE CORRIG√â:"""

        try:
            messages = [
                {
                    "role": "system",
                    "content": "Tu es un expert en d√©bogage JavaScript. R√©ponds uniquement avec du code corrig√©."
                },
                {
                    "role": "user",
                    "content": fix_prompt
                }
            ]
            
            response = self.mistral_client.chat_completion(
                messages=messages,
                model=self.mistral_model,
                max_tokens=2000,
                temperature=0.3  # Moins cr√©atif, plus pr√©cis
            )
            
            fixed_code = response.choices[0].message.content.strip()
            
            # Nettoyage
            import re
            fixed_code = re.sub(r'```[a-z]*\n?', '', fixed_code)
            fixed_code = re.sub(r'/\*.*?\*/', '', fixed_code, flags=re.DOTALL)
            
            print(f"   ‚úÖ Code corrig√©: {len(fixed_code)} caract√®res")
            return fixed_code
            
        except Exception as e:
            print(f"   ‚ùå Correction √©chou√©e: {e}")
            return None


# Instance globale
_generator = None

def init_hybrid_generator():
    """Initialise le g√©n√©rateur hybride"""
    global _generator
    if _generator is None:
        _generator = HybridAIGenerator()
    return _generator

def generate_hybrid_3d(prompt, object_type='object'):
    """Point d'entr√©e principal"""
    generator = init_hybrid_generator()
    return generator.generate(prompt, object_type)

def fix_broken_code(code, error, prompt):
    """Point d'entr√©e pour auto-correction"""
    generator = init_hybrid_generator()
    fixed = generator.fix_code_with_mistral(code, error, prompt)
    if fixed:
        return {'success': True, 'fixed_code': fixed}
    else:
        return {'success': False, 'error': 'Auto-correction impossible'}

